{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8400e966",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT for Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bff58dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages.\n",
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661b343",
   "metadata": {},
   "source": [
    "### Check availability of GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "561362ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98270ce5",
   "metadata": {},
   "source": [
    "### Get training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ef7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['O','B-DOMMER','I-DOMMER','B-ADVOKAT','I-ADVOKAT','B-LOV','I-LOV','B-RET','I-RET','B-PERSON','I-PERSON','B-ORGANISATION','I-ORGANISATION','B-LOKATION','I-LOKATION','B-DATO','I-DATO','B-SAGSEMNE','I-SAGSEMNE']\n",
    "label_encoding_dict = {'O': 0, 'B-DOMMER': 1, 'I-DOMMER': 2, 'B-ADVOKAT': 3, 'I-ADVOKAT': 4, 'B-LOV': 5, 'I-LOV': 6, 'B-RET': 7, 'I-RET': 8, 'B-PERSON': 9, 'I-PERSON': 10, 'B-ORGANISATION': 11,'I-ORGANISATION': 12, 'B-LOKATION': 13, 'I-LOKATION': 14, 'B-DATO': 15, 'I-DATO': 16, 'B-SAGSEMNE': 17, 'I-SAGSEMNE': 18}\n",
    "\n",
    "task = \"ner\" \n",
    "model_checkpoint = \"distilbert-base-multilingual-cased\"\n",
    "batch_size = 16\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    \n",
    "def get_tokens_and_ner_tags(filename):\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        split_list = [list(y) for x, y in itertools.groupby(lines, lambda z: z == '\\n') if not x]\n",
    "        tokens = [[x.split(' ')[0] for x in y] for y in split_list]\n",
    "        entities = []\n",
    "        for y in split_list:\n",
    "            doc = []\n",
    "            for x in y:\n",
    "                try:\n",
    "                    doc.append(x.split(' ')[3][:-1])\n",
    "                except IndexError:\n",
    "                    doc.append(x.split(' ')[2][:-1])\n",
    "            entities.append(doc)\n",
    "\n",
    "    return pd.DataFrame({'tokens': tokens, 'ner_tags': entities})\n",
    "  \n",
    "def get_token_dataset(filename):\n",
    "    df = get_tokens_and_ner_tags(filename)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    train_dataset = dataset['train']\n",
    "    test_dataset = dataset['test']\n",
    "\n",
    "    return (train_dataset, test_dataset)\n",
    "\n",
    "train_dataset, test_dataset = get_token_dataset('../data/danish_legal_ner_dataset.conll')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb87fb",
   "metadata": {},
   "source": [
    "### Tokenize datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609cb76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    label_all_tokens = False\n",
    "    tokenized_inputs = tokenizer(list(examples[\"tokens\"]), truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif label[word_idx] == '0':\n",
    "                label_ids.append(0)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_encoding_dict[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label_encoding_dict[label[word_idx]] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "        \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "train_tokenized_datasets = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_tokenized_datasets = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a528c13",
   "metadata": {},
   "source": [
    "### Fine-Tune BERT model for Named Entity Recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59075aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"test-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=30,\n",
    "    weight_decay=1e-5,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"]}\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_tokenized_datasets,\n",
    "    eval_dataset=test_tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853ed802",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea54a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced71a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('danish-ner-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa93b4d",
   "metadata": {},
   "source": [
    "### Predict entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('danish-ner-model')\n",
    "\n",
    "paragraph = '''Den 11. april 2014 holdt Østre Landsret møde i retsbygningen, Bredgade 59, København. Som dommere fungerede landsdommerne Mogens Kroman, Betina Juul Heldmann (kst.) og Andreas Bøgsted-Møller (kst.), førstnævnte som rettens formand. Som protokolfører fungerede Medarbejder ved retten. Der foretoges 5. afd. kære nr. B-975-14: Kærende, tidligere Far mod Indkærede, tidligere Mor Ingen var mødt eller indkaldt. Der fremlagdes kæreskrift af 29. marts 2014, hvorved Kærende, tidligere Far har påkæret Odense Rets kendelse af 24. marts 2014 (BS 3-390/2014), hvor det tiltrædes, at statsfor-valtningen har afvist at behandle Kærendes, tidligere Far anmodning om overførelse af for-ældremyndigheden over Barn 1 til sig, dommerens fremsendelsesbrev af 1. april 2014 og udskrift af retsbogen med den påkærede afgørelse. Indkærede har ikke udtalt sig vedrørende kæren. Byretten har ved sagens fremsendelse henholdt sig til den trufne afgørelse. De modtagne bilag var til stede. Efter votering afsagdes k e n d e l s e : Landsretten er enig i byrettens resultat og begrundelsen herfor. Landsretten stadfæster derfor byrettens afgørelse. T h i b e s t e m m e s : Byrettens kendelse stadfæstes. Ingen af parterne skal betale kæremålsomkostninger til den anden part. Retten hævet. '''\n",
    "tokens = tokenizer(paragraph)\n",
    "torch.tensor(tokens['input_ids']).unsqueeze(0).size()\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('danish-ner-model', num_labels=len(label_list))\n",
    "predictions = model.forward(input_ids=torch.tensor(tokens['input_ids']).unsqueeze(0), attention_mask=torch.tensor(tokens['attention_mask']).unsqueeze(0))\n",
    "predictions = torch.argmax(predictions.logits.squeeze(), axis=1)\n",
    "predictions = [label_list[i] for i in predictions]\n",
    "\n",
    "words = tokenizer.batch_decode(tokens['input_ids'])\n",
    "pd.DataFrame({'ner': predictions, 'words': words})\n",
    "pd.DataFrame({'ner': predictions, 'words': words}).to_csv('danish_legal_ner.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c0da67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ner')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d66191e947aa68313bb5c8e3aaa99a56fb09ab158f4d78dc49442376e9c349bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
